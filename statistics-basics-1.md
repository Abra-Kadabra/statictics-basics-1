## Основы статистики

## Часть I

### Генеральная совокупность и выборка

**Генеральная совокупность (ГС)** – совокупность всех объектов, относительно которых предполагается делать выводы.
**Репрезентативная выборка** – такая выборка из ГС, в которой распределение изучаемых признаков представлено с примерно такой же частотой как и в ГС.

#### Способы репрезентативной выборки 

- **Простая случайная выборка** (simple random sample)
- **Стратифицированная выборка** (stratified sample) – разделение ГС на страты (группы), из которых уже делается случайная выборка. Такой метод *повышает точность*, гарантируя что каждая страта будет представлена в выборке. Для разделения на страты необходимым условием является *однородность элементов внутри страты* и различия между элементами разных страт.
- **Групповая выборка** (cluster sample) – похожие группы (кластеры) выделяются из ГС, далее среди выбранных групп проводится случайная выборка. Такой метод *уменьшает накладные расходы*, благодаря тому что выборка производится из меньшей совокупности. Для разделения на кластеры необходимым условием является *однородность между кластерами*.


### Типы переменных

**Количественные** – измеряемые, выражаются числами:
- **Непрерывные** – выражаются целыми числами;
- **Дискретные** – выражаются действительными числами.

**Качественные** – неизмеряемые, не выражаются числами:
- **Ранговые** – можно только упорядочить, но ничего не известно об абсолютном значении;
- **Номинальные** – значения вовсе не содержат арифметического смысла.

### Меры центральной тенденции

**Мода** (mode) – значение признака, которое встречается наиболее часто.

**Медиана** (median) – значение признака, делящее упорядоченное множество данных пополам. В случае нечётного количества элементов множества медиана принимает значение среднего элемента, а в случае нечётного – среднее значение двух серединных элементов.

**Среднее значение** (mean, среднее арифметические) – сумма всех значений признака, делённое на количество признаков. Обозначается как $\overline X$ для выборки и как $\mu$ для ГС.

### Меры изменчивости

**Размах** (range) – разность между максимальным и минимальным значениями выборки. 
Крайне чувствителен к выбросам.

**Дисперсия** (variance) – средний квадрат отклонений значений признака от их средней величины. $$D[X] = Var(X) = \sigma ^{2} = \frac{\sum_{i=1}^{n} (x_{i} - M_{x})^2}{n}$$ *Среднеквадратичное отклонение* равно корню из дисперсии: $\sigma = \sqrt{D}$, обозначается как $\sigma$ для ГС и как $sd$ (standard deviation) для выборки.


### Квартили распределения, box-plot

**Квартили** (quartile) – три значения признака, делящие упорядоченное множество данных на 4 равные части. Второй квартиль это медиана.

**Интерквартильный размах** (interquartile range) – разность между третьим и первым квартилями.

<img src="/pics/box-plot.png" width="500">

**Box-plot** – вид диаграммы, показывающий медиану, первый и третий квартили, минимальное и максимальное значение выборки и выбросы.

### Нормальное распределение

- Унимодально;
- Симметрично;
- Отклонение наблюдений от среднего подчиняются определённому вероятностному закону.

**Z-преобразование** – преобразование данных стандартную Z-шкалу (Z-scores) со *средним значением = 0 и дисперсией = 1*. Для приведения к данному виду, необходимо из каждого наблюдения отнять среднее значение и поделить на стандартное отклонение: $Z_{i}=\frac{x_{i} - \overline{X}}{sd}$

<img src="/pics/68-95-99.7.png" width="500">

**Z-таблица** – таблица, в которой указано, какой процент значений падает ниже определенного Z-показателя в стандартном нормальном распределении.

### Центральная предельная теорема

ЦПТ – множество средних выборок из ГС будут иметь нормальное распределение. Причём среднее такого распределения будет близко к среднему ГС, а стандартная ошибка называется **стандартной ошибкой среднего** (se).

Зная среднеквадратичное отклонение ГС и размер выборки (N), можно рассчитать стандартную ошибку среднего: $$se = \frac{\sigma}{\sqrt{N}}$$Стандартная ошибка среднего - это среднеквадратическое отклонение распределения выборочных средних.

### Доверительные интервалы для среднего

Зная среднее по выборке мы можем с *некоторой степенью уверенности* сказать в каком интервале лежит среднее по ГС. Если взять некоторую выборку $X$ и вычислить её среднее $\overline X$ и стандартную ошибку среднего $se$, то мы сможем вычислить доверительный интервал, описывающий среднюю ГС с некоторым % доверия.

<img src="/pics/1.96is95.png" width="500">

В частности, для 95% доверительного интервала, расчёты будут выглядеть так: $$[\bar{X} - 1.96*se; \bar{X} + 1.96*se]$$где $1,96$ это количество сигм в нормальном распределении, необходимых чтобы охватить 95% значений этого распределения.

Таким образом, "95% доверительный интервал для среднего ГС" означает что среднее ГС принадлежит этому интервалу c 95% вероятностью.

### Идея статистического вывода, p-уровень значимости

<img src="/pics/pregnant-tests.jpg" width="500">

**Уровень значимости $\alpha$** – вероятность отвергнуть нулевую гипотезу, при условии что она верна (т.е. совершить ошибку первого рода).



**P-уровень значимости** (P-value) – минимальный уровень значимости при котором нулевая гипотеза может быть отвергнута.

## Часть II

### T-распределение

В ситуациях когда нам неизвестно $\sigma$ (стандартное отклонение в ГС), а значит нет возможности рассчитать $se$ (стандартную ошибку ГС), в наших расчётах будем использовать $sd$: $$\large t = \frac{\bar{X} - \mu}{\frac{sd}{\sqrt{n}}}$$!

<img src="/pics/t_distribution.png" width="500">

График плотности распределения Стьюдента, как и нормального распределения, симметричен и имеет форму колокола, но с более «тяжёлыми» хвостами. Чем больше степеней свободы тем график ближе к графику нормального распределения.

### Сравнение двух средних. t-критерий Стьюдента.

**t-критерий Стьюдента** – общее название для статистических тестов, в которых статистика имеет распределение Стьюдента. Наиболее часто t-критерии применяются для *проверки равенства средних значений* в двух выборках, когда нулевая гипотеза предполагает что средние равны. Для применения этого критерия необходимо чтобы исходные данные имели нормальное распределение. $$t = \frac{\bar{X_1} - \bar{X_2}}{se}$$$$  se = \sqrt{\frac{sd_1^2}{n_1} + \frac{sd_2^2}{n_2}}$$ 
### Проверка распределения на нормальность, qq-plot

**График квантиль-квантиль** (qq-plot) сравнивает квантили наблюдаемых данных с квантилями определённого распределения (в нашем случае нормального – normal qq-plot). Позволяет визуально оценить соответствие определённого распределения нормальному.

<img src="/pics/qq-plot.png" width="500">

В случае, когда наблюдаемые данные распределены не нормально, или же размер выборок слишком мал для применения t-критерия, для оценки различий между двумя независимыми выборками по уровню какого-либо признака, можно использовать **U-критерий Манна-Уитни**.

### Однофакторный дисперсионный анализ

**Однофакторный дисперсионный анализ** (однофакторный ANOVA от *AN*alysis *O*f *VA*riance) – метод статистического анализа данных, использующийся для определения наличия статистически значимых различий между двумя и более группами по одной независимой переменной.

**Независимая переменная** – такая переменная, что разделяет наши наблюдения на группы (номинативная переменная с несколькими градациями). 

**Зависимая переменная** – количественная переменная, по степени выраженности которой сравниваются группы.

### Множественные сравнения в ANOVA

Кумулятивная вероятность отклонить нулевую гипотезу растет с количеством попарных сравнений испытаний (т.е. растёт вероятность совершить ошибку первого рода). Для устранения этого эффекта вводится **поправка на множественную ошибку**.

**Поправка Бонферрони** предлагает уменьшить уровень статистической значимости результата $\alpha$ в количество раз, равное проведённым попарным сравнениям. Считается надёжным, но слишком консервативным и устаревшим методом, отсекающим слишком много потенциально достоверных гипотез. 

**Критерий Тьюки** предложен как более предпочтительный метод сгладить множественную ошибку. Детали метода/теста/поправки/критерия не приводятся.

### Многофакторный ANOVA

**Многофакторный дисперсионный анализ** позволяет проверить влияние нескольких факторов на зависимую переменную. Помимо пофакторного влияния позволяет выявить и значимое взаимодействие факторов.

При применении дисперсионного анализа стоит обращать внимание на:
- нормальность распределения зависимой переменной в группах;
- гомогенность дисперсий.

## Часть III

### Понятие корреляции

**Ковариация** – мера *линейной* изменчивости двух случайных величин.

Положительная ковариация – с возрастанием значений одной случайной величины, значения второй *также* имеет тенденцию к возрастанию. 
Отрицательная ковариация – с возрастанием значений одной случайной величины, значения вторая, *напротив*, имеет тенденцию к убыванию.

<img src="/pics/cov-corr.png" width="500">

$$cov(X, Y) = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{N - 1}$$
, где $N$ – количество случайных величин, а $1$ – количество степеней свободы.

По *абсолютному* значению ковариации *нельзя судить о том, насколько сильно величины взаимосвязаны*, так как масштаб ковариации зависит от их дисперсий. Значение ковариации можно нормировать, поделив её на произведение среднеквадратических отклонений (квадратных корней из дисперсии) случайных величин. Полученная величина называется **коэффициентом корреляции Пирсона**, принимающим значения от $–1$ до $1$:
$$r(x, y) = \frac{cov(x, y)}{\sigma_x\sigma_y} = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sqrt{\sum{(x_i - \bar{x})^2}\sum{(y_i - \bar{y})^2}}}$$
### Регрессия с одной независимой переменной 

**Регрессионный анализ** – набор статистических методов влияния одной или нескольких переменных $X_1, X_2, ..., X_p$ на зависимую переменную $Y$.

**Линейная регрессия** (Linear regression) – самый простой вариант регрессионного анализа, при помощи которой можно исследовать взаимосвязь двух переменных. 

В общем виде функция линейной регрессии записывается как: 

$$y = b_0 +b_1 x$$



$b_0$ (intercept) – значение пересечения с осью $Y$

$b_1$ (slope) – наклон линии регрессии

<img src="/pics/least-squares.jpg" width="500">

**Метод наименьших квадратов** (МНК) – способ нахождения оптимальных параметров линейной регрессии, такой, что сумма квадратов ошибок (остатков) была бы минимальна. $$b_1 = \frac{sd_y}{sd_x}r_{xy}$$

$$b_0 = \bar{Y} - b_1\bar{X}$$

### Регрессионный анализ с несколькими независимыми переменными

**Множественная регрессия** (Multiple Regression) – вид регрессионного анализа, позволяющий исследовать влияние нескольких независимых переменный на зависимую.

Отличие между простой и множественными линейными регрессиями в том, что вместо линии регрессии в ней используется гиперплоскость, уравнение которой имеет вид:
$$y = b_0 + b_1 x_1 + b_2 x_2 + ... + b_n x_n$$
Как и в простой линейной регрессии, параметры модели $b_n$ вычисляются при помощи метода наименьших квадратов.